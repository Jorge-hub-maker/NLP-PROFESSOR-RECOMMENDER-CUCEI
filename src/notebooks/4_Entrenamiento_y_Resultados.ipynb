{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 1. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optimizaciones GPU\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "print(\" Librerías cargadas - VERSIÓN CON PESOS OPTIMIZADOS IIC\")\n",
    "print(f\" CUDA disponible: {torch.cuda.is_available()}\")\n",
    "\n",
    "\n",
    "# SISTEMA DE PESOS OPTIMIZADO PARA INGENIERÍA EN INFORMÁTICA Y COMPUTACIÓN\n",
    "PESOS_DEPARTAMENTOS_IIC = {\n",
    "    'DEPARTAMENTO DE CIENCIAS COMPUTACIONALES': 1.0, \n",
    "    'DEPTO. DE CIENCIAS COMPUTACIONALES': 1.0, \n",
    "    'CIENCIAS COMPUTACIONALES': 1.0, \n",
    "    'DEPARTAMENTO DE INNOVACIÓN BASADA EN LA INFORMACIÓN Y EL CONOCIMIENTO': 0.95, \n",
    "    'INNOVACION BASADA EN LA INFORMACION Y EL CONOCIMIENTO': 0.95, \n",
    "    'DEPTO. DE INNOVACIÓN BASADA EN LA INFORMACIÓN Y EL CONOCIMIENTO': 0.95, \n",
    "    'DEPARTAMENTO DE MATEMÁTICAS': 0.88, \n",
    "    'DEPTO. DE MATEMATICAS': 0.88, \n",
    "    'MATEMATICAS': 0.88, \n",
    "    'DEPTO. DE MATEMÁTICAS': 0.88, \n",
    "    'DEPARTAMENTO DE INGENIERÍA ELECTRO-FOTÓNICA': 0.82, \n",
    "    'INGENIERIA ELECTRO-FOTONICA': 0.82, \n",
    "    'DEPTO. DE INGENIERÍA ELECTRO-FOTÓNICA': 0.82, \n",
    "    'DEPARTAMENTO DE FÍSICA': 0.75, \n",
    "    'DEPTO. DE FISICA': 0.75, \n",
    "    'FISICA': 0.75, \n",
    "    'DEPARTAMENTO DE INGENIERÍA INDUSTRIAL': 0.7, \n",
    "    'INGENIERIA INDUSTRIAL': 0.7, \n",
    "    'DEPTO. DE INGENIERÍA INDUSTRIAL': 0.7, \n",
    "    'DEPARTAMENTO DE INGENIERÍA DE PROYECTOS': 0.65, \n",
    "    'INGENIERIA DE PROYECTOS': 0.65, \n",
    "    'DEPTO. DE INGENIERÍA DE PROYECTOS': 0.65, \n",
    "    'DEPARTAMENTO DE INGENIERÍA MECÁNICA ELÉCTRICA': 0.62, \n",
    "    'INGENIERIA MECANICA ELECTRICA': 0.62, \n",
    "    'DEPTO. DE INGENIERÍA MECÁNICA ELÉCTRICA': 0.62, \n",
    "    'DEPARTAMENTO DE INGENIERÍA CIVIL Y TOPOGRAFÍA': 0.55, \n",
    "    'INGENIERIA CIVIL Y TOPOGRAFIA': 0.55, \n",
    "    'DEPTO. DE INGENIERÍA CIVIL Y TOPOGRAFÍA': 0.55, \n",
    "    'DEPARTAMENTO DE INGENIERÍA QUÍMICA': 0.5, \n",
    "    'INGENIERIA QUIMICA': 0.5, \n",
    "    'DEPTO. DE INGENIERÍA QUÍMICA': 0.5, \n",
    "    'DEPARTAMENTO DE BIOINGENIERÍA TRASLACIONAL': 0.45, \n",
    "    'DEPTO DE BIOINGENIERIA TRASLACIONAL': 0.45, \n",
    "    'BIOINGENIERIA TRASLACIONAL': 0.45, \n",
    "    'DEPARTAMENTO DE QUÍMICA': 0.35, \n",
    "    'QUIMICA': 0.35, \n",
    "    'DEPTO. DE QUÍMICA': 0.35, \n",
    "    'DEPARTAMENTO DE FARMACOBIOLOGÍA': 0.3, \n",
    "    'FARMACOBIOLOGIA': 0.3, \n",
    "    'DEPTO. DE FARMACOBIOLOGÍA': 0.3, \n",
    "    'DEPARTAMENTO DE MADERA, CELULOSA Y PAPEL': 0.25, \n",
    "    'MADERA CELULOSA Y PAPEL': 0.25, \n",
    "    'DEPTO. DE MADERA, CELULOSA Y PAPEL': 0.25, \n",
    "    'No encontrado': 0.1, \n",
    "    'División no encontrada': 0.1, \n",
    "    'DEPTO. NO ENCONTRADO': 0.1, \n",
    "    'Sin departamento': 0.1\n",
    "}\n",
    "\n",
    "PESOS_DIVISIONES = {\n",
    "    'DIVISION DE TECNOLOGIAS PARA LA INTEGRACION CIBER-HUMANA': 1.0, \n",
    "    'División de Tecnologías para la Integración Ciber-Humana': 1.0, \n",
    "    'TECNOLOGIAS PARA LA INTEGRACION CIBER-HUMANA': 1.0, \n",
    "    'DIVISION DE CIENCIAS BASICAS': 0.65, \n",
    "    'División de Ciencias Básicas': 0.65, \n",
    "    'CIENCIAS BASICAS': 0.65, \n",
    "    'DIVISION DE INGENIERIAS': 0.7, \n",
    "    'División de Ingenierías': 0.7, \n",
    "    'INGENIERIAS': 0.7, \n",
    "    'Sin división': 0.1, \n",
    "    'División no encontrada': 0.1\n",
    "}\n",
    "\n",
    "def get_optimal_weight_iic(departamento, division=None):\n",
    "    \"\"\"\n",
    "    Función optimizada para obtener pesos específicos de IIC\n",
    "    Combina peso de departamento (70%) + división (30%)\n",
    "    \"\"\"\n",
    "    dept_key = str(departamento).upper().strip()\n",
    "    dept_weight = PESOS_DEPARTAMENTOS_IIC.get(dept_key, 0.10)\n",
    "    \n",
    "    if division:\n",
    "        div_key = str(division).upper().strip()\n",
    "        div_weight = PESOS_DIVISIONES.get(div_key, 0.10)\n",
    "        # Combinación ponderada: mayor peso al departamento\n",
    "        final_weight = 0.7 * dept_weight + 0.3 * div_weight\n",
    "    else:\n",
    "        final_weight = dept_weight\n",
    "        \n",
    "    return min(final_weight, 1.0)  # Cap máximo 1.0\n",
    "\n",
    "print(\" Sistema de pesos optimizado IIC cargado\")\n",
    "print(f\" Total departamentos: {len(PESOS_DEPARTAMENTOS_IIC)}\")\n",
    "print(f\" Total divisiones: {len(PESOS_DIVISIONES)}\")\n",
    "\n",
    "\n",
    "class OptimizedProfessorModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo optimizado con sistema de pesos IIC mejorado\n",
    "    Incluye procesamiento de divisiones y pesos dinámicos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim=256, dropout=0.3):\n",
    "        super(OptimizedProfessorModel, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Encoder mejorado con más capacidad\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Procesador de pesos departamentales \n",
    "        self.dept_processor = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Procesador de pesos de división \n",
    "        self.division_processor = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        \n",
    "        # Fusión de características mejorada\n",
    "        fusion_input_dim = hidden_dim + 64 + 32  \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Cabezas de predicción optimizadas\n",
    "        self.sentiment_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 3)  \n",
    "        )\n",
    "        \n",
    "        self.rating_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)  \n",
    "        )\n",
    "        \n",
    "        # Inicialización de pesos optimizada\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.BatchNorm1d):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, embeddings, dept_weights, div_weights=None):\n",
    "        # Codificar embeddings de texto\n",
    "        encoded = self.encoder(embeddings)\n",
    "        \n",
    "        # Procesar pesos departamentales\n",
    "        dept_features = self.dept_processor(dept_weights)\n",
    "        \n",
    "        # Procesar pesos de división (si disponible)\n",
    "        if div_weights is not None:\n",
    "            div_features = self.division_processor(div_weights)\n",
    "        else:\n",
    "            # Crear características neutras de división\n",
    "            div_features = torch.zeros(embeddings.size(0), 32, device=embeddings.device)\n",
    "        \n",
    "        # Fusionar todas las características\n",
    "        combined = torch.cat([encoded, dept_features, div_features], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # Predicciones independientes\n",
    "        sentiment_logits = self.sentiment_head(fused)\n",
    "        rating_pred = self.rating_head(fused)\n",
    "        \n",
    "        # Rating final con aplicación dinámica de pesos\n",
    "        # Usar tanto peso departamental como divisional\n",
    "        total_weight = dept_weights\n",
    "        if div_weights is not None:\n",
    "            total_weight = 0.7 * dept_weights + 0.3 * div_weights\n",
    "        \n",
    "        weighted_rating = rating_pred * total_weight.expand_as(rating_pred)\n",
    "        final_rating = torch.sigmoid(weighted_rating)  # Normalizar 0-1\n",
    "        \n",
    "        return {\n",
    "            'sentiment_logits': sentiment_logits,\n",
    "            'rating_pred': rating_pred,\n",
    "            'final_rating': final_rating,\n",
    "            'dept_weights': dept_weights,\n",
    "            'div_weights': div_weights,\n",
    "            'total_weights': total_weight\n",
    "        }\n",
    "\n",
    "print(\" Modelo optimizado IIC definido\")\n",
    "\n",
    "\n",
    "class EnhancedProfesorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset mejorado con sistema de pesos IIC y procesamiento de divisiones\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, ratings, departments, divisions, comments):\n",
    "        print(\"🧹 Creando dataset optimizado IIC...\")\n",
    "        \n",
    "        # Limpieza robusta de datos\n",
    "        clean_data = []\n",
    "        for i, rating in enumerate(ratings):\n",
    "            try:\n",
    "                if rating is not None and not pd.isna(rating):\n",
    "                    rating_float = float(rating)\n",
    "                    if 1 <= rating_float <= 10:\n",
    "                        clean_data.append({\n",
    "                            'index': i,\n",
    "                            'rating': rating_float,\n",
    "                            'embedding': embeddings[i],\n",
    "                            'department': departments[i] if i < len(departments) else 'No encontrado',\n",
    "                            'division': divisions[i] if divisions and i < len(divisions) else 'Sin división',\n",
    "                            'comment': comments[i] if i < len(comments) else ''\n",
    "                        })\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        \n",
    "        # Si no hay ratings válidos, crear sintéticos\n",
    "        if len(clean_data) == 0:\n",
    "            print(\" Creando ratings sintéticos basados en longitud de comentarios...\")\n",
    "            for i in range(len(embeddings)):\n",
    "                comment_len = len(str(comments[i])) if i < len(comments) and comments[i] else 50\n",
    "                synthetic_rating = 5.0 + (comment_len / 100) * 3 + np.random.normal(0, 0.8)\n",
    "                synthetic_rating = max(1.0, min(10.0, synthetic_rating))\n",
    "                \n",
    "                clean_data.append({\n",
    "                    'index': i,\n",
    "                    'rating': synthetic_rating,\n",
    "                    'embedding': embeddings[i],\n",
    "                    'department': departments[i] if i < len(departments) else 'No encontrado',\n",
    "                    'division': divisions[i] if divisions and i < len(divisions) else 'Sin división',\n",
    "                    'comment': comments[i] if i < len(comments) else f'Comentario sintético {i}'\n",
    "                })\n",
    "        \n",
    "        # Extraer datos procesados\n",
    "        self.embeddings = torch.FloatTensor([d['embedding'] for d in clean_data])\n",
    "        self.ratings = torch.FloatTensor([d['rating'] for d in clean_data])\n",
    "        self.departments = [d['department'] for d in clean_data]\n",
    "        self.divisions = [d['division'] for d in clean_data]\n",
    "        self.comments = [d['comment'] for d in clean_data]\n",
    "        \n",
    "        # CALCULAR PESOS OPTIMIZADOS IIC\n",
    "        print(\" Calculando pesos optimizados IIC...\")\n",
    "        self.dept_weights = []\n",
    "        self.div_weights = []\n",
    "        \n",
    "        for i, dept in enumerate(self.departments):\n",
    "            div = self.divisions[i] if i < len(self.divisions) else None\n",
    "            \n",
    "            # Peso combinado usando la función optimizada\n",
    "            combined_weight = get_optimal_weight_iic(dept, div)\n",
    "            self.dept_weights.append(combined_weight)\n",
    "            \n",
    "            # Peso específico de división\n",
    "            div_key = str(div).upper().strip() if div else 'Sin división'\n",
    "            div_weight = PESOS_DIVISIONES.get(div_key, 0.1)\n",
    "            self.div_weights.append(div_weight)\n",
    "        \n",
    "        self.dept_weights = torch.FloatTensor(self.dept_weights).unsqueeze(1)\n",
    "        self.div_weights = torch.FloatTensor(self.div_weights).unsqueeze(1)\n",
    "        \n",
    "        # Normalizar ratings para entrenamiento (0-1)\n",
    "        self.normalized_ratings = ((self.ratings - 1) / 9).unsqueeze(1)\n",
    "        \n",
    "        # Labels de sentimiento mejorados\n",
    "        self.sentiment_labels = torch.LongTensor([\n",
    "            2 if r >= 8.0 else 1 if r >= 6.0 else 0 for r in self.ratings\n",
    "        ])\n",
    "        \n",
    "        # Estadísticas del dataset\n",
    "        print(f\"  Dataset optimizado IIC creado:\")\n",
    "        print(f\"  • Samples válidos: {len(self.ratings)}\")\n",
    "        print(f\"  • Rating promedio: {torch.mean(self.ratings):.2f} ± {torch.std(self.ratings):.2f}\")\n",
    "        print(f\"  • Peso departamental promedio: {torch.mean(self.dept_weights):.3f}\")\n",
    "        print(f\"  • Peso división promedio: {torch.mean(self.div_weights):.3f}\")\n",
    "        print(f\"  • Range pesos: {torch.min(self.dept_weights):.2f} - {torch.max(self.dept_weights):.2f}\")\n",
    "        \n",
    "        # Distribución de sentimientos\n",
    "        sentiment_counts = torch.bincount(self.sentiment_labels)\n",
    "        print(f\"  • Sentimientos - Neg: {sentiment_counts[0]}, Neu: {sentiment_counts[1]}, Pos: {sentiment_counts[2]}\")\n",
    "        \n",
    "        # Análisis por departamentos principales\n",
    "        dept_analysis = {}\n",
    "        for dept in set(self.departments):\n",
    "            if 'COMPUTACIONALES' in str(dept).upper():\n",
    "                indices = [i for i, d in enumerate(self.departments) if d == dept]\n",
    "                if indices:\n",
    "                    avg_rating = torch.mean(self.ratings[indices])\n",
    "                    avg_weight = torch.mean(self.dept_weights[indices])\n",
    "                    dept_analysis[dept] = {'count': len(indices), 'avg_rating': avg_rating.item(), 'weight': avg_weight.item()}\n",
    "        \n",
    "        if dept_analysis:\n",
    "            print(f\"  • Análisis Ciencias Computacionales:\")\n",
    "            for dept, stats in dept_analysis.items():\n",
    "                print(f\"    - {dept}: {stats['count']} samples, rating {stats['avg_rating']:.2f}, peso {stats['weight']:.2f}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'embedding': self.embeddings[idx],\n",
    "            'rating': self.ratings[idx],\n",
    "            'normalized_rating': self.normalized_ratings[idx],\n",
    "            'dept_weight': self.dept_weights[idx],\n",
    "            'div_weight': self.div_weights[idx],\n",
    "            'sentiment_label': self.sentiment_labels[idx],\n",
    "            'department': self.departments[idx],\n",
    "            'division': self.divisions[idx],\n",
    "            'comment': self.comments[idx]\n",
    "        }\n",
    "\n",
    "print(\" Dataset optimizado IIC definido\")\n",
    "\n",
    "# %%\n",
    "def optimized_train_epoch(model, train_loader, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Entrenamiento optimizado con balance dinámico de pérdidas\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    sentiment_loss_total = 0\n",
    "    rating_loss_total = 0\n",
    "    weight_regularization_total = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Datos a GPU\n",
    "        embeddings = batch['embedding'].to(device, non_blocking=True)\n",
    "        dept_weights = batch['dept_weight'].to(device, non_blocking=True)\n",
    "        div_weights = batch['div_weight'].to(device, non_blocking=True)\n",
    "        sentiment_labels = batch['sentiment_label'].to(device, non_blocking=True)\n",
    "        normalized_ratings = batch['normalized_rating'].to(device, non_blocking=True)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(embeddings, dept_weights, div_weights)\n",
    "        \n",
    "        # Pérdidas principales\n",
    "        sentiment_loss = nn.CrossEntropyLoss()(outputs['sentiment_logits'], sentiment_labels)\n",
    "        rating_loss = nn.MSELoss()(outputs['final_rating'], normalized_ratings)\n",
    "        \n",
    "        # Regularización de pesos (penalizar pesos extremos)\n",
    "        weight_reg = torch.mean(torch.abs(outputs['total_weights'] - 0.5)) * 0.01\n",
    "        \n",
    "        # Balance dinámico de pérdidas por época\n",
    "        if epoch < 5:\n",
    "            # Primeras épocas: enfoque en rating\n",
    "            rating_weight = 0.8\n",
    "            sentiment_weight = 0.2\n",
    "        elif epoch < 10:\n",
    "            # Épocas medias: balance\n",
    "            rating_weight = 0.6\n",
    "            sentiment_weight = 0.4\n",
    "        else:\n",
    "            # Épocas finales: refinamiento\n",
    "            rating_weight = 0.7\n",
    "            sentiment_weight = 0.3\n",
    "        \n",
    "        # Pérdida total combinada\n",
    "        total_batch_loss = (rating_weight * rating_loss + \n",
    "                           sentiment_weight * sentiment_loss + \n",
    "                           weight_reg)\n",
    "        \n",
    "        # Backward pass\n",
    "        total_batch_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Acumular métricas\n",
    "        total_loss += total_batch_loss.item()\n",
    "        sentiment_loss_total += sentiment_loss.item()\n",
    "        rating_loss_total += rating_loss.item()\n",
    "        weight_regularization_total += weight_reg.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Progreso cada 10 batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"     Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                  f\"Total: {total_batch_loss.item():.4f} | \"\n",
    "                  f\"Rating: {rating_loss.item():.4f} | \"\n",
    "                  f\"Sentiment: {sentiment_loss.item():.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'total_loss': total_loss / num_batches,\n",
    "        'rating_loss': rating_loss_total / num_batches,\n",
    "        'sentiment_loss': sentiment_loss_total / num_batches,\n",
    "        'weight_reg': weight_regularization_total / num_batches\n",
    "    }\n",
    "\n",
    "def optimized_validate(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Validación optimizada con métricas detalladas IIC\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    sentiment_preds = []\n",
    "    sentiment_actuals = []\n",
    "    weight_analysis = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            embeddings = batch['embedding'].to(device, non_blocking=True)\n",
    "            dept_weights = batch['dept_weight'].to(device, non_blocking=True)\n",
    "            div_weights = batch['div_weight'].to(device, non_blocking=True)\n",
    "            sentiment_labels = batch['sentiment_label'].to(device, non_blocking=True)\n",
    "            normalized_ratings = batch['normalized_rating'].to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = model(embeddings, dept_weights, div_weights)\n",
    "            \n",
    "            # Pérdida de validación\n",
    "            rating_loss = nn.MSELoss()(outputs['final_rating'], normalized_ratings)\n",
    "            sentiment_loss = nn.CrossEntropyLoss()(outputs['sentiment_logits'], sentiment_labels)\n",
    "            batch_loss = 0.7 * rating_loss + 0.3 * sentiment_loss\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "            \n",
    "            # Recopilar predicciones\n",
    "            predictions.extend(outputs['final_rating'].cpu().numpy())\n",
    "            actuals.extend(normalized_ratings.cpu().numpy())\n",
    "            \n",
    "            # Sentimientos\n",
    "            sentiment_pred = torch.softmax(outputs['sentiment_logits'], dim=1)\n",
    "            sentiment_preds.extend(torch.argmax(sentiment_pred, dim=1).cpu().numpy())\n",
    "            sentiment_actuals.extend(sentiment_labels.cpu().numpy())\n",
    "            \n",
    "            # Análisis de pesos\n",
    "            weight_analysis.extend(outputs['total_weights'].cpu().numpy())\n",
    "    \n",
    "    # Calcular métricas\n",
    "    predictions = np.array(predictions).flatten()\n",
    "    actuals = np.array(actuals).flatten()\n",
    "    weight_analysis = np.array(weight_analysis).flatten()\n",
    "    \n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    correlation = np.corrcoef(actuals, predictions)[0,1] if len(set(predictions)) > 1 else 0.0\n",
    "    sentiment_accuracy = np.mean(np.array(sentiment_preds) == np.array(sentiment_actuals))\n",
    "    \n",
    "    return {\n",
    "        'val_loss': total_loss / len(val_loader),\n",
    "        'mse': mse,\n",
    "        'rmse': np.sqrt(mse),\n",
    "        'correlation': correlation,\n",
    "        'sentiment_accuracy': sentiment_accuracy,\n",
    "        'predictions': predictions,\n",
    "        'actuals': actuals,\n",
    "        'avg_weight': np.mean(weight_analysis),\n",
    "        'weight_std': np.std(weight_analysis)\n",
    "    }\n",
    "\n",
    "print(\" Funciones de entrenamiento optimizadas IIC definidas\")\n",
    "\n",
    "\n",
    "# CARGA DE DATOS Y CONFIGURACIÓN\n",
    "def load_data_with_divisions(embeddings_dir):\n",
    "    \"\"\"\n",
    "    Carga datos incluyendo información de divisiones si está disponible\n",
    "    \"\"\"\n",
    "    files = list(Path(embeddings_dir).glob(\"*_complete.pkl\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No se encontraron archivos de embeddings\")\n",
    "    \n",
    "    latest_file = max(files, key=os.path.getctime)\n",
    "    print(f\" Cargando: {latest_file}\")\n",
    "    \n",
    "    with open(latest_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    embeddings = data['embeddings']\n",
    "    data_dict = data['data']\n",
    "    \n",
    "    ratings = data_dict.get('ratings', [])\n",
    "    departments = data_dict.get('departments', [])\n",
    "    comments = data_dict.get('original_comments', data_dict.get('comments', []))\n",
    "    \n",
    "    # Intentar obtener divisiones (pueden no estar disponibles)\n",
    "    divisions = data_dict.get('divisions', data_dict.get('division', None))\n",
    "    \n",
    "    print(f\"📊 Diagnóstico de carga:\")\n",
    "    print(f\"  • Embeddings: {len(embeddings)} muestras\")\n",
    "    print(f\"  • Ratings: {len(ratings)} valores\")\n",
    "    print(f\"  • Departamentos: {len(departments)} valores\")\n",
    "    print(f\"  • Divisiones: {len(divisions) if divisions else 'No disponibles'}\")\n",
    "    print(f\"  • Comentarios: {len(comments)} valores\")\n",
    "    \n",
    "    return embeddings, ratings, departments, divisions, comments\n",
    "\n",
    "# CONFIGURACIÓN DE RUTAS\n",
    "EMBEDDINGS_DIR = r\"# === NOTE: Replace with local path ===\"\n",
    "MODELS_DIR = r\"# === NOTE: Replace with local path ===\"\n",
    "RESULTS_DIR = r\"# === NOTE: Replace with local path ===\"\n",
    "\n",
    "# Crear directorios\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"🔄 Cargando datos...\")\n",
    "embeddings, ratings, departments, divisions, comments = load_data_with_divisions(EMBEDDINGS_DIR)\n",
    "\n",
    "# Crear dataset optimizado\n",
    "print(\" Creando dataset optimizado IIC...\")\n",
    "dataset = EnhancedProfesorDataset(embeddings, ratings, departments, divisions, comments)\n",
    "\n",
    "# Split balanceado\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size], \n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# DataLoaders optimizados\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32,\n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\" Datasets optimizados IIC:\")\n",
    "print(f\"  • Train: {len(train_dataset)} samples\")\n",
    "print(f\"  • Validation: {len(val_dataset)} samples\")\n",
    "\n",
    "\n",
    "# INICIALIZACIÓN DEL MODELO OPTIMIZADO\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = OptimizedProfessorModel(\n",
    "    embedding_dim=embeddings.shape[1],\n",
    "    hidden_dim=256,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\" Modelo optimizado IIC en {device}\")\n",
    "print(f\"Parámetros totales: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Optimizador con configuración mejorada\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=0.001,\n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Scheduler más sofisticado\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='max',  # Maximizar correlación\n",
    "    factor=0.5, \n",
    "    patience=3, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\" Optimizador y scheduler configurados\")\n",
    "\n",
    "# %%\n",
    "# ENTRENAMIENTO OPTIMIZADO IIC\n",
    "print(\" ENTRENAMIENTO OPTIMIZADO IIC\")\n",
    "print(\" Objetivo: Correlación >0.75 con pesos IIC especializados\")\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "best_correlation = -1.0\n",
    "best_val_loss = float('inf')\n",
    "patience = 0\n",
    "MAX_PATIENCE = 8\n",
    "\n",
    "# Tracking de métricas\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "correlations = []\n",
    "weight_analyses = []\n",
    "\n",
    "start_time = datetime.now()\n",
    "timestamp = start_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n Época {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Entrenar\n",
    "    train_metrics = optimized_train_epoch(model, train_loader, optimizer, device, epoch)\n",
    "    \n",
    "    # Validar\n",
    "    val_metrics = optimized_validate(model, val_loader, device)\n",
    "    \n",
    "    # Scheduler basado en correlación\n",
    "    scheduler.step(val_metrics['correlation'])\n",
    "    \n",
    "    # Tracking de métricas\n",
    "    train_losses.append(train_metrics['total_loss'])\n",
    "    val_losses.append(val_metrics['val_loss'])\n",
    "    correlations.append(val_metrics['correlation'])\n",
    "    weight_analyses.append({\n",
    "        'avg_weight': val_metrics['avg_weight'],\n",
    "        'weight_std': val_metrics['weight_std']\n",
    "    })\n",
    "    \n",
    "    # Mostrar métricas detalladas\n",
    "    print(f\"  🔸 Train Loss: {train_metrics['total_loss']:.4f}\")\n",
    "    print(f\"    - Rating: {train_metrics['rating_loss']:.4f}\")\n",
    "    print(f\"    - Sentiment: {train_metrics['sentiment_loss']:.4f}\")\n",
    "    print(f\"    - Weight Reg: {train_metrics['weight_reg']:.4f}\")\n",
    "    print(f\"  🔸 Val Loss: {val_metrics['val_loss']:.4f}\")\n",
    "    print(f\"  🔸 MSE: {val_metrics['mse']:.4f} | RMSE: {val_metrics['rmse']:.4f}\")\n",
    "    print(f\"  🔸 Correlación: {val_metrics['correlation']:.4f}\")\n",
    "    print(f\"  🔸 Sentiment Acc: {val_metrics['sentiment_accuracy']:.4f}\")\n",
    "    print(f\"  🔸 Peso promedio: {val_metrics['avg_weight']:.3f} ± {val_metrics['weight_std']:.3f}\")\n",
    "    print(f\"  🔸 LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Guardar mejor modelo\n",
    "    if val_metrics['correlation'] > best_correlation:\n",
    "        best_correlation = val_metrics['correlation']\n",
    "        best_val_loss = val_metrics['val_loss']\n",
    "        patience = 0\n",
    "        \n",
    "        # Guardar modelo optimizado\n",
    "        model_state = {\n",
    "            'state_dict': model.state_dict(),\n",
    "            'model_class': 'OptimizedProfessorModel',\n",
    "            'config': {\n",
    "                'embedding_dim': embeddings.shape[1],\n",
    "                'hidden_dim': 256,\n",
    "                'dropout': 0.3\n",
    "            },\n",
    "            'pesos_departamentos_iic': PESOS_DEPARTAMENTOS_IIC,\n",
    "            'pesos_divisiones': PESOS_DIVISIONES,\n",
    "            'metrics': val_metrics,\n",
    "            'epoch': epoch + 1,\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        best_model_path = os.path.join(MODELS_DIR, f'best_iic_model_{timestamp}.pth')\n",
    "        torch.save(model_state, best_model_path)\n",
    "        \n",
    "        print(f\"   NUEVO MEJOR MODELO IIC - Correlación: {best_correlation:.4f}\")\n",
    "        print(f\"     Guardado en: {best_model_path}\")\n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience >= MAX_PATIENCE:\n",
    "        print(f\"   Early stopping - Sin mejora en {MAX_PATIENCE} épocas\")\n",
    "        break\n",
    "    \n",
    "    # Tiempo transcurrido\n",
    "    elapsed = datetime.now() - start_time\n",
    "    print(f\"   Tiempo transcurrido: {elapsed}\")\n",
    "\n",
    "total_time = datetime.now() - start_time\n",
    "\n",
    "print(f\"\\n ENTRENAMIENTO IIC COMPLETADO\")\n",
    "print(f\" Tiempo total: {total_time}\")\n",
    "print(f\" Mejor correlación: {best_correlation:.4f}\")\n",
    "print(f\" Épocas completadas: {epoch + 1}/{NUM_EPOCHS}\")\n",
    "\n",
    "# %%\n",
    "# EVALUACIÓN FINAL Y GUARDADO COMPLETO\n",
    "print(\" Evaluación final y guardado completo...\")\n",
    "\n",
    "# Cargar mejor modelo\n",
    "best_model_path = os.path.join(MODELS_DIR, f'best_iic_model_{timestamp}.pth')\n",
    "model_checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(model_checkpoint['state_dict'])\n",
    "\n",
    "# Evaluación final completa\n",
    "final_metrics = optimized_validate(model, val_loader, device)\n",
    "\n",
    "print(f\"\\n MÉTRICAS FINALES OPTIMIZADAS IIC:\")\n",
    "print(f\"  • MSE: {final_metrics['mse']:.4f}\")\n",
    "print(f\"  • RMSE: {final_metrics['rmse']:.4f}\")\n",
    "print(f\"  • Correlación: {final_metrics['correlation']:.4f}\")\n",
    "print(f\"  • Sentiment Accuracy: {final_metrics['sentiment_accuracy']:.4f}\")\n",
    "print(f\"  • Peso Promedio: {final_metrics['avg_weight']:.3f}\")\n",
    "print(f\"  • Desviación Pesos: {final_metrics['weight_std']:.3f}\")\n",
    "\n",
    "# Guardado completo en Resultados\n",
    "results_complete = {\n",
    "    'sistema': 'Red Neuronal Optimizada IIC',\n",
    "    'timestamp': timestamp,\n",
    "    'tiempo_entrenamiento': str(total_time),\n",
    "    'configuracion': {\n",
    "        'modelo': 'OptimizedProfessorModel',\n",
    "        'embedding_dim': embeddings.shape[1],\n",
    "        'hidden_dim': 256,\n",
    "        'epochs_total': NUM_EPOCHS,\n",
    "        'epochs_completadas': epoch + 1,\n",
    "        'mejor_epoch': epoch + 1 - patience\n",
    "    },\n",
    "    'sistema_pesos': {\n",
    "        'tipo': 'Optimizado para Ingeniería en Informática y Computación',\n",
    "        'departamentos_mapeados': len(PESOS_DEPARTAMENTOS_IIC),\n",
    "        'divisiones_mapeadas': len(PESOS_DIVISIONES),\n",
    "        'peso_maximo': max(PESOS_DEPARTAMENTOS_IIC.values()),\n",
    "        'peso_minimo': min(PESOS_DEPARTAMENTOS_IIC.values())\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'total_samples': len(dataset),\n",
    "        'train_samples': len(train_dataset),\n",
    "        'val_samples': len(val_dataset),\n",
    "        'tiene_divisiones': divisions is not None\n",
    "    },\n",
    "    'metricas_finales': {\n",
    "        'mse': float(final_metrics['mse']),\n",
    "        'rmse': float(final_metrics['rmse']),\n",
    "        'correlacion': float(final_metrics['correlation']),\n",
    "        'sentiment_accuracy': float(final_metrics['sentiment_accuracy']),\n",
    "        'peso_promedio': float(final_metrics['avg_weight']),\n",
    "        'peso_desviacion': float(final_metrics['weight_std'])\n",
    "    },\n",
    "    'rendimiento_entrenamiento': {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'correlaciones': correlations,\n",
    "        'analisis_pesos': weight_analyses\n",
    "    },\n",
    "    'predicciones_detalladas': {\n",
    "        'predictions': [float(p) for p in final_metrics['predictions']],\n",
    "        'actuals': [float(a) for a in final_metrics['actuals']]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar resultados completos\n",
    "results_file = os.path.join(RESULTS_DIR, f\"resultados_completos_iic_{timestamp}.json\")\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_complete, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "# Modelo final para producción\n",
    "model_final = {\n",
    "    'modelo_completo': model_checkpoint,\n",
    "    'resultados': results_complete,\n",
    "    'instrucciones_uso': {\n",
    "        'carga_modelo': f\"torch.load('{best_model_path}')\",\n",
    "        'pesos_departamentos': 'PESOS_DEPARTAMENTOS_IIC',\n",
    "        'pesos_divisiones': 'PESOS_DIVISIONES',\n",
    "        'funcion_peso': 'get_optimal_weight_iic(dept, div)'\n",
    "    }\n",
    "}\n",
    "\n",
    "model_final_path = os.path.join(RESULTS_DIR, f\"modelo_produccion_iic_{timestamp}.pth\")\n",
    "torch.save(model_final, model_final_path)\n",
    "\n",
    "print(f\"\\n ARCHIVOS GENERADOS:\")\n",
    "print(f\"   Mejor modelo: {best_model_path}\")\n",
    "print(f\"   Resultados completos: {results_file}\")\n",
    "print(f\"   Modelo producción: {model_final_path}\")\n",
    "\n",
    "# Clasificación final del modelo\n",
    "if final_metrics['correlation'] > 0.80:\n",
    "    classification = \"🏆 EXCELENTE - Modelo de alta precisión\"\n",
    "elif final_metrics['correlation'] > 0.70:\n",
    "    classification = \"✅ MUY BUENO - Modelo funcional optimo\"\n",
    "elif final_metrics['correlation'] > 0.60:\n",
    "    classification = \"👍 BUENO - Modelo aceptable\"\n",
    "elif final_metrics['correlation'] > 0.45:\n",
    "    classification = \"⚠️ REGULAR - Necesita ajustes\"\n",
    "else:\n",
    "    classification = \"❌ BAJO - Requiere revisión\"\n",
    "\n",
    "print(f\"\\n CLASIFICACIÓN FINAL: {classification}\")\n",
    "print(f\" Correlación alcanzada: {final_metrics['correlation']:.4f}\")\n",
    "print(f\" Sistema de pesos IIC implementado exitosamente\")\n",
    "print(f\" ¡Red neuronal optimizada para Ingeniería en Informática lista!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" SISTEMA DE PESOS IIC INTEGRADO EXITOSAMENTE\")\n",
    "print(\" ¡TU RED NEURONAL ESTÁ OPTIMIZADA PARA INGENIERÍA INFORMÁTICA!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluación y Resultados del Sistema de Pesos IIC\n",
    "\n",
    "Presento los resultados finales del **Sistema de Pesos Optimizado IIC** aplicado al análisis de reseñas académicas en el área de **Ingeniería en Informática**.  \n",
    "Se entrenó un modelo neuronal con embeddings multilingües y un sistema de pesos especializado por **departamentos** y **divisiones**, optimizado para maximizar la correlación entre las predicciones y los datos reales.\n",
    "\n",
    "---\n",
    "\n",
    "##  Configuración del Experimento\n",
    "\n",
    "- **Embeddings cargados**: `profesores_embeddings_multilingual_robust_20250821_002728_complete.pkl`\n",
    "- **Muestras procesadas**: `1466`\n",
    "- **Muestras válidas (post-filtrado IIC)**: `461`\n",
    "- **Departamentos**: `47`\n",
    "- **Divisiones**: `11`\n",
    "- **Modelo entrenado en CUDA**: ✅ Sí\n",
    "- **Parámetros totales**: `815,844`\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Estadísticas del Dataset Optimizado IIC\n",
    "\n",
    "- **Rating promedio**: `7.16 ± 2.34`\n",
    "- **Peso departamental promedio**: `0.379`\n",
    "- **Peso división promedio**: `0.100`\n",
    "- **Rango de pesos**: `0.10 - 0.73`\n",
    "\n",
    "### 🔹 Distribución de Sentimientos\n",
    "- Negativo: `135`\n",
    "- Neutral: `105`\n",
    "- Positivo: `221`\n",
    "\n",
    "### 🔹 Ejemplo de análisis departamental\n",
    "- **Departamento de Ciencias Computacionales**:  \n",
    "  - Muestras: `105`  \n",
    "  - Rating promedio: `6.98`  \n",
    "  - Peso asignado: `0.73`  \n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Entrenamiento del Modelo Optimizado IIC\n",
    "\n",
    "- **Épocas totales**: `20`\n",
    "- **Tamaño del dataset**:  \n",
    "  - Train: `368` muestras  \n",
    "  - Validation: `93` muestras  \n",
    "\n",
    "📌 Durante el entrenamiento, la correlación entre predicciones y valores reales fue mejorando gradualmente:\n",
    "\n",
    "| Época | Loss Validación | RMSE  | Correlación | Accuracy Sentiment |\n",
    "|-------|-----------------|-------|-------------|--------------------|\n",
    "| 1     | 0.3803          | 0.3013 | -0.1332     | 0.48               |\n",
    "| 5     | 0.3692          | 0.2882 | 0.0882      | 0.49               |\n",
    "| 10    | 0.2939          | 0.2378 | 0.5594      | 0.59               |\n",
    "| 15    | 0.2684          | 0.2038 | 0.6571 🏆   | 0.64               |\n",
    "| 20    | 0.2565          | 0.2034 | 0.6568      | 0.65               |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Resultados Finales\n",
    "\n",
    "- **MSE**: `0.0415`\n",
    "- **RMSE**: `0.2038`\n",
    "- **Correlación**: `0.6571`\n",
    "- **Sentiment Accuracy**: `64.52%`\n",
    "- **Peso Promedio**: `0.298 ± 0.200`\n",
    "\n",
    "\n",
    "## 🎯 Conclusión\n",
    "\n",
    "El sistema de pesos IIC ha demostrado ser **efectivo en la optimización del análisis de reseñas académicas**, alcanzando una correlación de **0.6571**, lo que representa una **mejora significativa respecto a versiones previas**.  \n",
    "\n",
    "La precisión en la clasificación de sentimientos se estabilizó en torno al **65%**, lo cual valida la utilidad del sistema para **apoyar la toma de decisiones académicas en Ingeniería en Informática**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
