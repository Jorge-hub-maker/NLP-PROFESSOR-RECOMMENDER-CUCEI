{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 1. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optimizaciones GPU\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "print(\" Librer√≠as cargadas - VERSI√ìN CON PESOS OPTIMIZADOS IIC\")\n",
    "print(f\" CUDA disponible: {torch.cuda.is_available()}\")\n",
    "\n",
    "\n",
    "# SISTEMA DE PESOS OPTIMIZADO PARA INGENIER√çA EN INFORM√ÅTICA Y COMPUTACI√ìN\n",
    "PESOS_DEPARTAMENTOS_IIC = {\n",
    "    'DEPARTAMENTO DE CIENCIAS COMPUTACIONALES': 1.0, \n",
    "    'DEPTO. DE CIENCIAS COMPUTACIONALES': 1.0, \n",
    "    'CIENCIAS COMPUTACIONALES': 1.0, \n",
    "    'DEPARTAMENTO DE INNOVACI√ìN BASADA EN LA INFORMACI√ìN Y EL CONOCIMIENTO': 0.95, \n",
    "    'INNOVACION BASADA EN LA INFORMACION Y EL CONOCIMIENTO': 0.95, \n",
    "    'DEPTO. DE INNOVACI√ìN BASADA EN LA INFORMACI√ìN Y EL CONOCIMIENTO': 0.95, \n",
    "    'DEPARTAMENTO DE MATEM√ÅTICAS': 0.88, \n",
    "    'DEPTO. DE MATEMATICAS': 0.88, \n",
    "    'MATEMATICAS': 0.88, \n",
    "    'DEPTO. DE MATEM√ÅTICAS': 0.88, \n",
    "    'DEPARTAMENTO DE INGENIER√çA ELECTRO-FOT√ìNICA': 0.82, \n",
    "    'INGENIERIA ELECTRO-FOTONICA': 0.82, \n",
    "    'DEPTO. DE INGENIER√çA ELECTRO-FOT√ìNICA': 0.82, \n",
    "    'DEPARTAMENTO DE F√çSICA': 0.75, \n",
    "    'DEPTO. DE FISICA': 0.75, \n",
    "    'FISICA': 0.75, \n",
    "    'DEPARTAMENTO DE INGENIER√çA INDUSTRIAL': 0.7, \n",
    "    'INGENIERIA INDUSTRIAL': 0.7, \n",
    "    'DEPTO. DE INGENIER√çA INDUSTRIAL': 0.7, \n",
    "    'DEPARTAMENTO DE INGENIER√çA DE PROYECTOS': 0.65, \n",
    "    'INGENIERIA DE PROYECTOS': 0.65, \n",
    "    'DEPTO. DE INGENIER√çA DE PROYECTOS': 0.65, \n",
    "    'DEPARTAMENTO DE INGENIER√çA MEC√ÅNICA EL√âCTRICA': 0.62, \n",
    "    'INGENIERIA MECANICA ELECTRICA': 0.62, \n",
    "    'DEPTO. DE INGENIER√çA MEC√ÅNICA EL√âCTRICA': 0.62, \n",
    "    'DEPARTAMENTO DE INGENIER√çA CIVIL Y TOPOGRAF√çA': 0.55, \n",
    "    'INGENIERIA CIVIL Y TOPOGRAFIA': 0.55, \n",
    "    'DEPTO. DE INGENIER√çA CIVIL Y TOPOGRAF√çA': 0.55, \n",
    "    'DEPARTAMENTO DE INGENIER√çA QU√çMICA': 0.5, \n",
    "    'INGENIERIA QUIMICA': 0.5, \n",
    "    'DEPTO. DE INGENIER√çA QU√çMICA': 0.5, \n",
    "    'DEPARTAMENTO DE BIOINGENIER√çA TRASLACIONAL': 0.45, \n",
    "    'DEPTO DE BIOINGENIERIA TRASLACIONAL': 0.45, \n",
    "    'BIOINGENIERIA TRASLACIONAL': 0.45, \n",
    "    'DEPARTAMENTO DE QU√çMICA': 0.35, \n",
    "    'QUIMICA': 0.35, \n",
    "    'DEPTO. DE QU√çMICA': 0.35, \n",
    "    'DEPARTAMENTO DE FARMACOBIOLOG√çA': 0.3, \n",
    "    'FARMACOBIOLOGIA': 0.3, \n",
    "    'DEPTO. DE FARMACOBIOLOG√çA': 0.3, \n",
    "    'DEPARTAMENTO DE MADERA, CELULOSA Y PAPEL': 0.25, \n",
    "    'MADERA CELULOSA Y PAPEL': 0.25, \n",
    "    'DEPTO. DE MADERA, CELULOSA Y PAPEL': 0.25, \n",
    "    'No encontrado': 0.1, \n",
    "    'Divisi√≥n no encontrada': 0.1, \n",
    "    'DEPTO. NO ENCONTRADO': 0.1, \n",
    "    'Sin departamento': 0.1\n",
    "}\n",
    "\n",
    "PESOS_DIVISIONES = {\n",
    "    'DIVISION DE TECNOLOGIAS PARA LA INTEGRACION CIBER-HUMANA': 1.0, \n",
    "    'Divisi√≥n de Tecnolog√≠as para la Integraci√≥n Ciber-Humana': 1.0, \n",
    "    'TECNOLOGIAS PARA LA INTEGRACION CIBER-HUMANA': 1.0, \n",
    "    'DIVISION DE CIENCIAS BASICAS': 0.65, \n",
    "    'Divisi√≥n de Ciencias B√°sicas': 0.65, \n",
    "    'CIENCIAS BASICAS': 0.65, \n",
    "    'DIVISION DE INGENIERIAS': 0.7, \n",
    "    'Divisi√≥n de Ingenier√≠as': 0.7, \n",
    "    'INGENIERIAS': 0.7, \n",
    "    'Sin divisi√≥n': 0.1, \n",
    "    'Divisi√≥n no encontrada': 0.1\n",
    "}\n",
    "\n",
    "def get_optimal_weight_iic(departamento, division=None):\n",
    "    \"\"\"\n",
    "    Funci√≥n optimizada para obtener pesos espec√≠ficos de IIC\n",
    "    Combina peso de departamento (70%) + divisi√≥n (30%)\n",
    "    \"\"\"\n",
    "    dept_key = str(departamento).upper().strip()\n",
    "    dept_weight = PESOS_DEPARTAMENTOS_IIC.get(dept_key, 0.10)\n",
    "    \n",
    "    if division:\n",
    "        div_key = str(division).upper().strip()\n",
    "        div_weight = PESOS_DIVISIONES.get(div_key, 0.10)\n",
    "        # Combinaci√≥n ponderada: mayor peso al departamento\n",
    "        final_weight = 0.7 * dept_weight + 0.3 * div_weight\n",
    "    else:\n",
    "        final_weight = dept_weight\n",
    "        \n",
    "    return min(final_weight, 1.0)  # Cap m√°ximo 1.0\n",
    "\n",
    "print(\" Sistema de pesos optimizado IIC cargado\")\n",
    "print(f\" Total departamentos: {len(PESOS_DEPARTAMENTOS_IIC)}\")\n",
    "print(f\" Total divisiones: {len(PESOS_DIVISIONES)}\")\n",
    "\n",
    "\n",
    "class OptimizedProfessorModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo optimizado con sistema de pesos IIC mejorado\n",
    "    Incluye procesamiento de divisiones y pesos din√°micos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim=256, dropout=0.3):\n",
    "        super(OptimizedProfessorModel, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Encoder mejorado con m√°s capacidad\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Procesador de pesos departamentales \n",
    "        self.dept_processor = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Procesador de pesos de divisi√≥n \n",
    "        self.division_processor = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        \n",
    "        # Fusi√≥n de caracter√≠sticas mejorada\n",
    "        fusion_input_dim = hidden_dim + 64 + 32  \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Cabezas de predicci√≥n optimizadas\n",
    "        self.sentiment_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 3)  \n",
    "        )\n",
    "        \n",
    "        self.rating_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)  \n",
    "        )\n",
    "        \n",
    "        # Inicializaci√≥n de pesos optimizada\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.BatchNorm1d):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, embeddings, dept_weights, div_weights=None):\n",
    "        # Codificar embeddings de texto\n",
    "        encoded = self.encoder(embeddings)\n",
    "        \n",
    "        # Procesar pesos departamentales\n",
    "        dept_features = self.dept_processor(dept_weights)\n",
    "        \n",
    "        # Procesar pesos de divisi√≥n (si disponible)\n",
    "        if div_weights is not None:\n",
    "            div_features = self.division_processor(div_weights)\n",
    "        else:\n",
    "            # Crear caracter√≠sticas neutras de divisi√≥n\n",
    "            div_features = torch.zeros(embeddings.size(0), 32, device=embeddings.device)\n",
    "        \n",
    "        # Fusionar todas las caracter√≠sticas\n",
    "        combined = torch.cat([encoded, dept_features, div_features], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # Predicciones independientes\n",
    "        sentiment_logits = self.sentiment_head(fused)\n",
    "        rating_pred = self.rating_head(fused)\n",
    "        \n",
    "        # Rating final con aplicaci√≥n din√°mica de pesos\n",
    "        # Usar tanto peso departamental como divisional\n",
    "        total_weight = dept_weights\n",
    "        if div_weights is not None:\n",
    "            total_weight = 0.7 * dept_weights + 0.3 * div_weights\n",
    "        \n",
    "        weighted_rating = rating_pred * total_weight.expand_as(rating_pred)\n",
    "        final_rating = torch.sigmoid(weighted_rating)  # Normalizar 0-1\n",
    "        \n",
    "        return {\n",
    "            'sentiment_logits': sentiment_logits,\n",
    "            'rating_pred': rating_pred,\n",
    "            'final_rating': final_rating,\n",
    "            'dept_weights': dept_weights,\n",
    "            'div_weights': div_weights,\n",
    "            'total_weights': total_weight\n",
    "        }\n",
    "\n",
    "print(\" Modelo optimizado IIC definido\")\n",
    "\n",
    "\n",
    "class EnhancedProfesorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset mejorado con sistema de pesos IIC y procesamiento de divisiones\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, ratings, departments, divisions, comments):\n",
    "        print(\"üßπ Creando dataset optimizado IIC...\")\n",
    "        \n",
    "        # Limpieza robusta de datos\n",
    "        clean_data = []\n",
    "        for i, rating in enumerate(ratings):\n",
    "            try:\n",
    "                if rating is not None and not pd.isna(rating):\n",
    "                    rating_float = float(rating)\n",
    "                    if 1 <= rating_float <= 10:\n",
    "                        clean_data.append({\n",
    "                            'index': i,\n",
    "                            'rating': rating_float,\n",
    "                            'embedding': embeddings[i],\n",
    "                            'department': departments[i] if i < len(departments) else 'No encontrado',\n",
    "                            'division': divisions[i] if divisions and i < len(divisions) else 'Sin divisi√≥n',\n",
    "                            'comment': comments[i] if i < len(comments) else ''\n",
    "                        })\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        \n",
    "        # Si no hay ratings v√°lidos, crear sint√©ticos\n",
    "        if len(clean_data) == 0:\n",
    "            print(\" Creando ratings sint√©ticos basados en longitud de comentarios...\")\n",
    "            for i in range(len(embeddings)):\n",
    "                comment_len = len(str(comments[i])) if i < len(comments) and comments[i] else 50\n",
    "                synthetic_rating = 5.0 + (comment_len / 100) * 3 + np.random.normal(0, 0.8)\n",
    "                synthetic_rating = max(1.0, min(10.0, synthetic_rating))\n",
    "                \n",
    "                clean_data.append({\n",
    "                    'index': i,\n",
    "                    'rating': synthetic_rating,\n",
    "                    'embedding': embeddings[i],\n",
    "                    'department': departments[i] if i < len(departments) else 'No encontrado',\n",
    "                    'division': divisions[i] if divisions and i < len(divisions) else 'Sin divisi√≥n',\n",
    "                    'comment': comments[i] if i < len(comments) else f'Comentario sint√©tico {i}'\n",
    "                })\n",
    "        \n",
    "        # Extraer datos procesados\n",
    "        self.embeddings = torch.FloatTensor([d['embedding'] for d in clean_data])\n",
    "        self.ratings = torch.FloatTensor([d['rating'] for d in clean_data])\n",
    "        self.departments = [d['department'] for d in clean_data]\n",
    "        self.divisions = [d['division'] for d in clean_data]\n",
    "        self.comments = [d['comment'] for d in clean_data]\n",
    "        \n",
    "        # CALCULAR PESOS OPTIMIZADOS IIC\n",
    "        print(\" Calculando pesos optimizados IIC...\")\n",
    "        self.dept_weights = []\n",
    "        self.div_weights = []\n",
    "        \n",
    "        for i, dept in enumerate(self.departments):\n",
    "            div = self.divisions[i] if i < len(self.divisions) else None\n",
    "            \n",
    "            # Peso combinado usando la funci√≥n optimizada\n",
    "            combined_weight = get_optimal_weight_iic(dept, div)\n",
    "            self.dept_weights.append(combined_weight)\n",
    "            \n",
    "            # Peso espec√≠fico de divisi√≥n\n",
    "            div_key = str(div).upper().strip() if div else 'Sin divisi√≥n'\n",
    "            div_weight = PESOS_DIVISIONES.get(div_key, 0.1)\n",
    "            self.div_weights.append(div_weight)\n",
    "        \n",
    "        self.dept_weights = torch.FloatTensor(self.dept_weights).unsqueeze(1)\n",
    "        self.div_weights = torch.FloatTensor(self.div_weights).unsqueeze(1)\n",
    "        \n",
    "        # Normalizar ratings para entrenamiento (0-1)\n",
    "        self.normalized_ratings = ((self.ratings - 1) / 9).unsqueeze(1)\n",
    "        \n",
    "        # Labels de sentimiento mejorados\n",
    "        self.sentiment_labels = torch.LongTensor([\n",
    "            2 if r >= 8.0 else 1 if r >= 6.0 else 0 for r in self.ratings\n",
    "        ])\n",
    "        \n",
    "        # Estad√≠sticas del dataset\n",
    "        print(f\"  Dataset optimizado IIC creado:\")\n",
    "        print(f\"  ‚Ä¢ Samples v√°lidos: {len(self.ratings)}\")\n",
    "        print(f\"  ‚Ä¢ Rating promedio: {torch.mean(self.ratings):.2f} ¬± {torch.std(self.ratings):.2f}\")\n",
    "        print(f\"  ‚Ä¢ Peso departamental promedio: {torch.mean(self.dept_weights):.3f}\")\n",
    "        print(f\"  ‚Ä¢ Peso divisi√≥n promedio: {torch.mean(self.div_weights):.3f}\")\n",
    "        print(f\"  ‚Ä¢ Range pesos: {torch.min(self.dept_weights):.2f} - {torch.max(self.dept_weights):.2f}\")\n",
    "        \n",
    "        # Distribuci√≥n de sentimientos\n",
    "        sentiment_counts = torch.bincount(self.sentiment_labels)\n",
    "        print(f\"  ‚Ä¢ Sentimientos - Neg: {sentiment_counts[0]}, Neu: {sentiment_counts[1]}, Pos: {sentiment_counts[2]}\")\n",
    "        \n",
    "        # An√°lisis por departamentos principales\n",
    "        dept_analysis = {}\n",
    "        for dept in set(self.departments):\n",
    "            if 'COMPUTACIONALES' in str(dept).upper():\n",
    "                indices = [i for i, d in enumerate(self.departments) if d == dept]\n",
    "                if indices:\n",
    "                    avg_rating = torch.mean(self.ratings[indices])\n",
    "                    avg_weight = torch.mean(self.dept_weights[indices])\n",
    "                    dept_analysis[dept] = {'count': len(indices), 'avg_rating': avg_rating.item(), 'weight': avg_weight.item()}\n",
    "        \n",
    "        if dept_analysis:\n",
    "            print(f\"  ‚Ä¢ An√°lisis Ciencias Computacionales:\")\n",
    "            for dept, stats in dept_analysis.items():\n",
    "                print(f\"    - {dept}: {stats['count']} samples, rating {stats['avg_rating']:.2f}, peso {stats['weight']:.2f}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'embedding': self.embeddings[idx],\n",
    "            'rating': self.ratings[idx],\n",
    "            'normalized_rating': self.normalized_ratings[idx],\n",
    "            'dept_weight': self.dept_weights[idx],\n",
    "            'div_weight': self.div_weights[idx],\n",
    "            'sentiment_label': self.sentiment_labels[idx],\n",
    "            'department': self.departments[idx],\n",
    "            'division': self.divisions[idx],\n",
    "            'comment': self.comments[idx]\n",
    "        }\n",
    "\n",
    "print(\" Dataset optimizado IIC definido\")\n",
    "\n",
    "# %%\n",
    "def optimized_train_epoch(model, train_loader, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Entrenamiento optimizado con balance din√°mico de p√©rdidas\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    sentiment_loss_total = 0\n",
    "    rating_loss_total = 0\n",
    "    weight_regularization_total = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Datos a GPU\n",
    "        embeddings = batch['embedding'].to(device, non_blocking=True)\n",
    "        dept_weights = batch['dept_weight'].to(device, non_blocking=True)\n",
    "        div_weights = batch['div_weight'].to(device, non_blocking=True)\n",
    "        sentiment_labels = batch['sentiment_label'].to(device, non_blocking=True)\n",
    "        normalized_ratings = batch['normalized_rating'].to(device, non_blocking=True)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(embeddings, dept_weights, div_weights)\n",
    "        \n",
    "        # P√©rdidas principales\n",
    "        sentiment_loss = nn.CrossEntropyLoss()(outputs['sentiment_logits'], sentiment_labels)\n",
    "        rating_loss = nn.MSELoss()(outputs['final_rating'], normalized_ratings)\n",
    "        \n",
    "        # Regularizaci√≥n de pesos (penalizar pesos extremos)\n",
    "        weight_reg = torch.mean(torch.abs(outputs['total_weights'] - 0.5)) * 0.01\n",
    "        \n",
    "        # Balance din√°mico de p√©rdidas por √©poca\n",
    "        if epoch < 5:\n",
    "            # Primeras √©pocas: enfoque en rating\n",
    "            rating_weight = 0.8\n",
    "            sentiment_weight = 0.2\n",
    "        elif epoch < 10:\n",
    "            # √âpocas medias: balance\n",
    "            rating_weight = 0.6\n",
    "            sentiment_weight = 0.4\n",
    "        else:\n",
    "            # √âpocas finales: refinamiento\n",
    "            rating_weight = 0.7\n",
    "            sentiment_weight = 0.3\n",
    "        \n",
    "        # P√©rdida total combinada\n",
    "        total_batch_loss = (rating_weight * rating_loss + \n",
    "                           sentiment_weight * sentiment_loss + \n",
    "                           weight_reg)\n",
    "        \n",
    "        # Backward pass\n",
    "        total_batch_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Acumular m√©tricas\n",
    "        total_loss += total_batch_loss.item()\n",
    "        sentiment_loss_total += sentiment_loss.item()\n",
    "        rating_loss_total += rating_loss.item()\n",
    "        weight_regularization_total += weight_reg.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Progreso cada 10 batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"     Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                  f\"Total: {total_batch_loss.item():.4f} | \"\n",
    "                  f\"Rating: {rating_loss.item():.4f} | \"\n",
    "                  f\"Sentiment: {sentiment_loss.item():.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'total_loss': total_loss / num_batches,\n",
    "        'rating_loss': rating_loss_total / num_batches,\n",
    "        'sentiment_loss': sentiment_loss_total / num_batches,\n",
    "        'weight_reg': weight_regularization_total / num_batches\n",
    "    }\n",
    "\n",
    "def optimized_validate(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Validaci√≥n optimizada con m√©tricas detalladas IIC\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    sentiment_preds = []\n",
    "    sentiment_actuals = []\n",
    "    weight_analysis = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            embeddings = batch['embedding'].to(device, non_blocking=True)\n",
    "            dept_weights = batch['dept_weight'].to(device, non_blocking=True)\n",
    "            div_weights = batch['div_weight'].to(device, non_blocking=True)\n",
    "            sentiment_labels = batch['sentiment_label'].to(device, non_blocking=True)\n",
    "            normalized_ratings = batch['normalized_rating'].to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = model(embeddings, dept_weights, div_weights)\n",
    "            \n",
    "            # P√©rdida de validaci√≥n\n",
    "            rating_loss = nn.MSELoss()(outputs['final_rating'], normalized_ratings)\n",
    "            sentiment_loss = nn.CrossEntropyLoss()(outputs['sentiment_logits'], sentiment_labels)\n",
    "            batch_loss = 0.7 * rating_loss + 0.3 * sentiment_loss\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "            \n",
    "            # Recopilar predicciones\n",
    "            predictions.extend(outputs['final_rating'].cpu().numpy())\n",
    "            actuals.extend(normalized_ratings.cpu().numpy())\n",
    "            \n",
    "            # Sentimientos\n",
    "            sentiment_pred = torch.softmax(outputs['sentiment_logits'], dim=1)\n",
    "            sentiment_preds.extend(torch.argmax(sentiment_pred, dim=1).cpu().numpy())\n",
    "            sentiment_actuals.extend(sentiment_labels.cpu().numpy())\n",
    "            \n",
    "            # An√°lisis de pesos\n",
    "            weight_analysis.extend(outputs['total_weights'].cpu().numpy())\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    predictions = np.array(predictions).flatten()\n",
    "    actuals = np.array(actuals).flatten()\n",
    "    weight_analysis = np.array(weight_analysis).flatten()\n",
    "    \n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    correlation = np.corrcoef(actuals, predictions)[0,1] if len(set(predictions)) > 1 else 0.0\n",
    "    sentiment_accuracy = np.mean(np.array(sentiment_preds) == np.array(sentiment_actuals))\n",
    "    \n",
    "    return {\n",
    "        'val_loss': total_loss / len(val_loader),\n",
    "        'mse': mse,\n",
    "        'rmse': np.sqrt(mse),\n",
    "        'correlation': correlation,\n",
    "        'sentiment_accuracy': sentiment_accuracy,\n",
    "        'predictions': predictions,\n",
    "        'actuals': actuals,\n",
    "        'avg_weight': np.mean(weight_analysis),\n",
    "        'weight_std': np.std(weight_analysis)\n",
    "    }\n",
    "\n",
    "print(\" Funciones de entrenamiento optimizadas IIC definidas\")\n",
    "\n",
    "\n",
    "# CARGA DE DATOS Y CONFIGURACI√ìN\n",
    "def load_data_with_divisions(embeddings_dir):\n",
    "    \"\"\"\n",
    "    Carga datos incluyendo informaci√≥n de divisiones si est√° disponible\n",
    "    \"\"\"\n",
    "    files = list(Path(embeddings_dir).glob(\"*_complete.pkl\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No se encontraron archivos de embeddings\")\n",
    "    \n",
    "    latest_file = max(files, key=os.path.getctime)\n",
    "    print(f\" Cargando: {latest_file}\")\n",
    "    \n",
    "    with open(latest_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    embeddings = data['embeddings']\n",
    "    data_dict = data['data']\n",
    "    \n",
    "    ratings = data_dict.get('ratings', [])\n",
    "    departments = data_dict.get('departments', [])\n",
    "    comments = data_dict.get('original_comments', data_dict.get('comments', []))\n",
    "    \n",
    "    # Intentar obtener divisiones (pueden no estar disponibles)\n",
    "    divisions = data_dict.get('divisions', data_dict.get('division', None))\n",
    "    \n",
    "    print(f\"üìä Diagn√≥stico de carga:\")\n",
    "    print(f\"  ‚Ä¢ Embeddings: {len(embeddings)} muestras\")\n",
    "    print(f\"  ‚Ä¢ Ratings: {len(ratings)} valores\")\n",
    "    print(f\"  ‚Ä¢ Departamentos: {len(departments)} valores\")\n",
    "    print(f\"  ‚Ä¢ Divisiones: {len(divisions) if divisions else 'No disponibles'}\")\n",
    "    print(f\"  ‚Ä¢ Comentarios: {len(comments)} valores\")\n",
    "    \n",
    "    return embeddings, ratings, departments, divisions, comments\n",
    "\n",
    "# CONFIGURACI√ìN DE RUTAS\n",
    "EMBEDDINGS_DIR = r\"# === NOTE: Replace with local path ===\"\n",
    "MODELS_DIR = r\"# === NOTE: Replace with local path ===\"\n",
    "RESULTS_DIR = r\"# === NOTE: Replace with local path ===\"\n",
    "\n",
    "# Crear directorios\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"üîÑ Cargando datos...\")\n",
    "embeddings, ratings, departments, divisions, comments = load_data_with_divisions(EMBEDDINGS_DIR)\n",
    "\n",
    "# Crear dataset optimizado\n",
    "print(\" Creando dataset optimizado IIC...\")\n",
    "dataset = EnhancedProfesorDataset(embeddings, ratings, departments, divisions, comments)\n",
    "\n",
    "# Split balanceado\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size], \n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# DataLoaders optimizados\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32,\n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\" Datasets optimizados IIC:\")\n",
    "print(f\"  ‚Ä¢ Train: {len(train_dataset)} samples\")\n",
    "print(f\"  ‚Ä¢ Validation: {len(val_dataset)} samples\")\n",
    "\n",
    "\n",
    "# INICIALIZACI√ìN DEL MODELO OPTIMIZADO\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = OptimizedProfessorModel(\n",
    "    embedding_dim=embeddings.shape[1],\n",
    "    hidden_dim=256,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\" Modelo optimizado IIC en {device}\")\n",
    "print(f\"Par√°metros totales: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Optimizador con configuraci√≥n mejorada\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=0.001,\n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Scheduler m√°s sofisticado\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='max',  # Maximizar correlaci√≥n\n",
    "    factor=0.5, \n",
    "    patience=3, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\" Optimizador y scheduler configurados\")\n",
    "\n",
    "# %%\n",
    "# ENTRENAMIENTO OPTIMIZADO IIC\n",
    "print(\" ENTRENAMIENTO OPTIMIZADO IIC\")\n",
    "print(\" Objetivo: Correlaci√≥n >0.75 con pesos IIC especializados\")\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "best_correlation = -1.0\n",
    "best_val_loss = float('inf')\n",
    "patience = 0\n",
    "MAX_PATIENCE = 8\n",
    "\n",
    "# Tracking de m√©tricas\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "correlations = []\n",
    "weight_analyses = []\n",
    "\n",
    "start_time = datetime.now()\n",
    "timestamp = start_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n √âpoca {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Entrenar\n",
    "    train_metrics = optimized_train_epoch(model, train_loader, optimizer, device, epoch)\n",
    "    \n",
    "    # Validar\n",
    "    val_metrics = optimized_validate(model, val_loader, device)\n",
    "    \n",
    "    # Scheduler basado en correlaci√≥n\n",
    "    scheduler.step(val_metrics['correlation'])\n",
    "    \n",
    "    # Tracking de m√©tricas\n",
    "    train_losses.append(train_metrics['total_loss'])\n",
    "    val_losses.append(val_metrics['val_loss'])\n",
    "    correlations.append(val_metrics['correlation'])\n",
    "    weight_analyses.append({\n",
    "        'avg_weight': val_metrics['avg_weight'],\n",
    "        'weight_std': val_metrics['weight_std']\n",
    "    })\n",
    "    \n",
    "    # Mostrar m√©tricas detalladas\n",
    "    print(f\"  üî∏ Train Loss: {train_metrics['total_loss']:.4f}\")\n",
    "    print(f\"    - Rating: {train_metrics['rating_loss']:.4f}\")\n",
    "    print(f\"    - Sentiment: {train_metrics['sentiment_loss']:.4f}\")\n",
    "    print(f\"    - Weight Reg: {train_metrics['weight_reg']:.4f}\")\n",
    "    print(f\"  üî∏ Val Loss: {val_metrics['val_loss']:.4f}\")\n",
    "    print(f\"  üî∏ MSE: {val_metrics['mse']:.4f} | RMSE: {val_metrics['rmse']:.4f}\")\n",
    "    print(f\"  üî∏ Correlaci√≥n: {val_metrics['correlation']:.4f}\")\n",
    "    print(f\"  üî∏ Sentiment Acc: {val_metrics['sentiment_accuracy']:.4f}\")\n",
    "    print(f\"  üî∏ Peso promedio: {val_metrics['avg_weight']:.3f} ¬± {val_metrics['weight_std']:.3f}\")\n",
    "    print(f\"  üî∏ LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Guardar mejor modelo\n",
    "    if val_metrics['correlation'] > best_correlation:\n",
    "        best_correlation = val_metrics['correlation']\n",
    "        best_val_loss = val_metrics['val_loss']\n",
    "        patience = 0\n",
    "        \n",
    "        # Guardar modelo optimizado\n",
    "        model_state = {\n",
    "            'state_dict': model.state_dict(),\n",
    "            'model_class': 'OptimizedProfessorModel',\n",
    "            'config': {\n",
    "                'embedding_dim': embeddings.shape[1],\n",
    "                'hidden_dim': 256,\n",
    "                'dropout': 0.3\n",
    "            },\n",
    "            'pesos_departamentos_iic': PESOS_DEPARTAMENTOS_IIC,\n",
    "            'pesos_divisiones': PESOS_DIVISIONES,\n",
    "            'metrics': val_metrics,\n",
    "            'epoch': epoch + 1,\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        best_model_path = os.path.join(MODELS_DIR, f'best_iic_model_{timestamp}.pth')\n",
    "        torch.save(model_state, best_model_path)\n",
    "        \n",
    "        print(f\"   NUEVO MEJOR MODELO IIC - Correlaci√≥n: {best_correlation:.4f}\")\n",
    "        print(f\"     Guardado en: {best_model_path}\")\n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience >= MAX_PATIENCE:\n",
    "        print(f\"   Early stopping - Sin mejora en {MAX_PATIENCE} √©pocas\")\n",
    "        break\n",
    "    \n",
    "    # Tiempo transcurrido\n",
    "    elapsed = datetime.now() - start_time\n",
    "    print(f\"   Tiempo transcurrido: {elapsed}\")\n",
    "\n",
    "total_time = datetime.now() - start_time\n",
    "\n",
    "print(f\"\\n ENTRENAMIENTO IIC COMPLETADO\")\n",
    "print(f\" Tiempo total: {total_time}\")\n",
    "print(f\" Mejor correlaci√≥n: {best_correlation:.4f}\")\n",
    "print(f\" √âpocas completadas: {epoch + 1}/{NUM_EPOCHS}\")\n",
    "\n",
    "# %%\n",
    "# EVALUACI√ìN FINAL Y GUARDADO COMPLETO\n",
    "print(\" Evaluaci√≥n final y guardado completo...\")\n",
    "\n",
    "# Cargar mejor modelo\n",
    "best_model_path = os.path.join(MODELS_DIR, f'best_iic_model_{timestamp}.pth')\n",
    "model_checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(model_checkpoint['state_dict'])\n",
    "\n",
    "# Evaluaci√≥n final completa\n",
    "final_metrics = optimized_validate(model, val_loader, device)\n",
    "\n",
    "print(f\"\\n M√âTRICAS FINALES OPTIMIZADAS IIC:\")\n",
    "print(f\"  ‚Ä¢ MSE: {final_metrics['mse']:.4f}\")\n",
    "print(f\"  ‚Ä¢ RMSE: {final_metrics['rmse']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Correlaci√≥n: {final_metrics['correlation']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Sentiment Accuracy: {final_metrics['sentiment_accuracy']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Peso Promedio: {final_metrics['avg_weight']:.3f}\")\n",
    "print(f\"  ‚Ä¢ Desviaci√≥n Pesos: {final_metrics['weight_std']:.3f}\")\n",
    "\n",
    "# Guardado completo en Resultados\n",
    "results_complete = {\n",
    "    'sistema': 'Red Neuronal Optimizada IIC',\n",
    "    'timestamp': timestamp,\n",
    "    'tiempo_entrenamiento': str(total_time),\n",
    "    'configuracion': {\n",
    "        'modelo': 'OptimizedProfessorModel',\n",
    "        'embedding_dim': embeddings.shape[1],\n",
    "        'hidden_dim': 256,\n",
    "        'epochs_total': NUM_EPOCHS,\n",
    "        'epochs_completadas': epoch + 1,\n",
    "        'mejor_epoch': epoch + 1 - patience\n",
    "    },\n",
    "    'sistema_pesos': {\n",
    "        'tipo': 'Optimizado para Ingenier√≠a en Inform√°tica y Computaci√≥n',\n",
    "        'departamentos_mapeados': len(PESOS_DEPARTAMENTOS_IIC),\n",
    "        'divisiones_mapeadas': len(PESOS_DIVISIONES),\n",
    "        'peso_maximo': max(PESOS_DEPARTAMENTOS_IIC.values()),\n",
    "        'peso_minimo': min(PESOS_DEPARTAMENTOS_IIC.values())\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'total_samples': len(dataset),\n",
    "        'train_samples': len(train_dataset),\n",
    "        'val_samples': len(val_dataset),\n",
    "        'tiene_divisiones': divisions is not None\n",
    "    },\n",
    "    'metricas_finales': {\n",
    "        'mse': float(final_metrics['mse']),\n",
    "        'rmse': float(final_metrics['rmse']),\n",
    "        'correlacion': float(final_metrics['correlation']),\n",
    "        'sentiment_accuracy': float(final_metrics['sentiment_accuracy']),\n",
    "        'peso_promedio': float(final_metrics['avg_weight']),\n",
    "        'peso_desviacion': float(final_metrics['weight_std'])\n",
    "    },\n",
    "    'rendimiento_entrenamiento': {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'correlaciones': correlations,\n",
    "        'analisis_pesos': weight_analyses\n",
    "    },\n",
    "    'predicciones_detalladas': {\n",
    "        'predictions': [float(p) for p in final_metrics['predictions']],\n",
    "        'actuals': [float(a) for a in final_metrics['actuals']]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar resultados completos\n",
    "results_file = os.path.join(RESULTS_DIR, f\"resultados_completos_iic_{timestamp}.json\")\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_complete, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "# Modelo final para producci√≥n\n",
    "model_final = {\n",
    "    'modelo_completo': model_checkpoint,\n",
    "    'resultados': results_complete,\n",
    "    'instrucciones_uso': {\n",
    "        'carga_modelo': f\"torch.load('{best_model_path}')\",\n",
    "        'pesos_departamentos': 'PESOS_DEPARTAMENTOS_IIC',\n",
    "        'pesos_divisiones': 'PESOS_DIVISIONES',\n",
    "        'funcion_peso': 'get_optimal_weight_iic(dept, div)'\n",
    "    }\n",
    "}\n",
    "\n",
    "model_final_path = os.path.join(RESULTS_DIR, f\"modelo_produccion_iic_{timestamp}.pth\")\n",
    "torch.save(model_final, model_final_path)\n",
    "\n",
    "print(f\"\\n ARCHIVOS GENERADOS:\")\n",
    "print(f\"   Mejor modelo: {best_model_path}\")\n",
    "print(f\"   Resultados completos: {results_file}\")\n",
    "print(f\"   Modelo producci√≥n: {model_final_path}\")\n",
    "\n",
    "# Clasificaci√≥n final del modelo\n",
    "if final_metrics['correlation'] > 0.80:\n",
    "    classification = \"üèÜ EXCELENTE - Modelo de alta precisi√≥n\"\n",
    "elif final_metrics['correlation'] > 0.70:\n",
    "    classification = \"‚úÖ MUY BUENO - Modelo funcional optimo\"\n",
    "elif final_metrics['correlation'] > 0.60:\n",
    "    classification = \"üëç BUENO - Modelo aceptable\"\n",
    "elif final_metrics['correlation'] > 0.45:\n",
    "    classification = \"‚ö†Ô∏è REGULAR - Necesita ajustes\"\n",
    "else:\n",
    "    classification = \"‚ùå BAJO - Requiere revisi√≥n\"\n",
    "\n",
    "print(f\"\\n CLASIFICACI√ìN FINAL: {classification}\")\n",
    "print(f\" Correlaci√≥n alcanzada: {final_metrics['correlation']:.4f}\")\n",
    "print(f\" Sistema de pesos IIC implementado exitosamente\")\n",
    "print(f\" ¬°Red neuronal optimizada para Ingenier√≠a en Inform√°tica lista!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" SISTEMA DE PESOS IIC INTEGRADO EXITOSAMENTE\")\n",
    "print(\" ¬°TU RED NEURONAL EST√Å OPTIMIZADA PARA INGENIER√çA INFORM√ÅTICA!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluaci√≥n y Resultados del Sistema de Pesos IIC\n",
    "\n",
    "Presento los resultados finales del **Sistema de Pesos Optimizado IIC** aplicado al an√°lisis de rese√±as acad√©micas en el √°rea de **Ingenier√≠a en Inform√°tica**.  \n",
    "Se entren√≥ un modelo neuronal con embeddings multiling√ºes y un sistema de pesos especializado por **departamentos** y **divisiones**, optimizado para maximizar la correlaci√≥n entre las predicciones y los datos reales.\n",
    "\n",
    "---\n",
    "\n",
    "##  Configuraci√≥n del Experimento\n",
    "\n",
    "- **Embeddings cargados**: `profesores_embeddings_multilingual_robust_20250821_002728_complete.pkl`\n",
    "- **Muestras procesadas**: `1466`\n",
    "- **Muestras v√°lidas (post-filtrado IIC)**: `461`\n",
    "- **Departamentos**: `47`\n",
    "- **Divisiones**: `11`\n",
    "- **Modelo entrenado en CUDA**: ‚úÖ S√≠\n",
    "- **Par√°metros totales**: `815,844`\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Estad√≠sticas del Dataset Optimizado IIC\n",
    "\n",
    "- **Rating promedio**: `7.16 ¬± 2.34`\n",
    "- **Peso departamental promedio**: `0.379`\n",
    "- **Peso divisi√≥n promedio**: `0.100`\n",
    "- **Rango de pesos**: `0.10 - 0.73`\n",
    "\n",
    "### üîπ Distribuci√≥n de Sentimientos\n",
    "- Negativo: `135`\n",
    "- Neutral: `105`\n",
    "- Positivo: `221`\n",
    "\n",
    "### üîπ Ejemplo de an√°lisis departamental\n",
    "- **Departamento de Ciencias Computacionales**:  \n",
    "  - Muestras: `105`  \n",
    "  - Rating promedio: `6.98`  \n",
    "  - Peso asignado: `0.73`  \n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Entrenamiento del Modelo Optimizado IIC\n",
    "\n",
    "- **√âpocas totales**: `20`\n",
    "- **Tama√±o del dataset**:  \n",
    "  - Train: `368` muestras  \n",
    "  - Validation: `93` muestras  \n",
    "\n",
    "üìå Durante el entrenamiento, la correlaci√≥n entre predicciones y valores reales fue mejorando gradualmente:\n",
    "\n",
    "| √âpoca | Loss Validaci√≥n | RMSE  | Correlaci√≥n | Accuracy Sentiment |\n",
    "|-------|-----------------|-------|-------------|--------------------|\n",
    "| 1     | 0.3803          | 0.3013 | -0.1332     | 0.48               |\n",
    "| 5     | 0.3692          | 0.2882 | 0.0882      | 0.49               |\n",
    "| 10    | 0.2939          | 0.2378 | 0.5594      | 0.59               |\n",
    "| 15    | 0.2684          | 0.2038 | 0.6571 üèÜ   | 0.64               |\n",
    "| 20    | 0.2565          | 0.2034 | 0.6568      | 0.65               |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Resultados Finales\n",
    "\n",
    "- **MSE**: `0.0415`\n",
    "- **RMSE**: `0.2038`\n",
    "- **Correlaci√≥n**: `0.6571`\n",
    "- **Sentiment Accuracy**: `64.52%`\n",
    "- **Peso Promedio**: `0.298 ¬± 0.200`\n",
    "\n",
    "\n",
    "## üéØ Conclusi√≥n\n",
    "\n",
    "El sistema de pesos IIC ha demostrado ser **efectivo en la optimizaci√≥n del an√°lisis de rese√±as acad√©micas**, alcanzando una correlaci√≥n de **0.6571**, lo que representa una **mejora significativa respecto a versiones previas**.  \n",
    "\n",
    "La precisi√≥n en la clasificaci√≥n de sentimientos se estabiliz√≥ en torno al **65%**, lo cual valida la utilidad del sistema para **apoyar la toma de decisiones acad√©micas en Ingenier√≠a en Inform√°tica**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
